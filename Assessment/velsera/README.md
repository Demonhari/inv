# Velsera â€“â€¯Researchâ€‘PaperÂ Analysis & Classification Pipeline

> Endâ€‘toâ€‘end demo: **textâ€‘inÂ â†’Â diseasesâ€‘out**
>
> *Preprocessâ€¯data â†’ train a baseline classifier â†’ LoRAâ€‘fineâ€‘tune â†’ run SciSpaCy NER â†’ clean disease list â†’ evaluate â†’ expose anÂ API.*

---

## 1Â .Â Whatâ€™s inside?

| path                  | purpose                                                                |
| --------------------- | ---------------------------------------------------------------------- |
| `data/raw/`           | 1â€¯000 PubMed abstracts in two folders: **Cancer** and **Nonâ€‘Cancer**   |
| `preprocess.py`       | builds **dataset.parquetÂ + 80/20 split** (`train.jsonl`, `test.jsonl`) |
| `baseline.py`         | DistilBERTâ€“based classifier (fullâ€‘fineâ€‘tune)                           |
| `train_lora.py`       | LoRA lowâ€‘rank adaptation on top of the baseline                        |
| `extract_diseases.py` | runs **SciSpaCyÂ (en\_ner\_bc5cdr\_md)** over abstracts                 |
| `clean_diseases.py`   | tiny helper to postâ€‘process NER output (blackâ€‘list, aliases, dedupe)   |
| `evaluate.py`         | accuracy / F1 / confusion matrix for both models                       |
| `app.py`              | FastAPI service â€“ classify abstracts & list diseasesâ€¯onâ€‘theâ€‘fly        |

---

## 2Â .Â QuickÂ startÂ ğŸ“¦

```bash
# â‘  clone + unzip the raw set -------------------------------------------------
git clone https://github.com/Demonhari/inv.git
cd Assesment/velsera
unzip data/raw/Dataset__1_.zip -d data/raw

# â‘¡ create a fresh Python â‰¤â€¯3.10 ------------------------------------------------
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# â‘¢ install required libraries --------------------------------------------------
pip install -r requirements.txt

# â‘£ (firstâ€‘time only) grab the SciSpaCy disease model --------------------------
#  â€“ we stay on spaCyÂ 3.4 (Pydanticâ€‘v1)
python -m pip install \
  https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bc5cdr_md-0.5.1.tar.gz

# â‘¤ run the pipeline ------------------------------------------------------------
python preprocess.py              # builds processed/ files
python baseline.py                # full fineâ€‘tune â€“ 3Â epochs (~6Â min CPU)
python train_lora.py              # LoRA â€“ 5Â epochs (~7Â min CPU)
python extract_diseases.py \
       data/processed/test.jsonl \
       output/diseases.jsonl      # + creates output/ if missing
python evaluate.py                # compare both models
```

Start the REST service:

```bash
uvicorn app:app --reload  # http://127.0.0.1:8000/docs
```

---

## 3Â .Â Detailed steps

### 3.1Â Preâ€‘processing

* strips PubMed headers, deâ€‘duplicates citations
* saves a single `dataset.parquet` (1000â€¯Ã—â€¯{"abstract","label"})
* fixed stratified 80/20 split so every script agrees on the same train/test

### 3.2Â Training options

| script          | trainableÂ params    | epochs | output                   |
| --------------- | ------------------- | ------ | ------------------------ |
| `baseline.py`   | 67.7â€¯M (full model) | Â 3Â     | `models/baseline/`       |
| `train_lora.py` | **0.81â€¯M** (1.2â€¯%)  | Â 5Â     | `models/lora_finetuned/` |

Both scripts pin **TransformersÂ â‰¥â€¯4.40**, **PyTorchâ€¯â‰¥â€¯2.0** and log loss every 0.5Â epoch.

### 3.3Â Disease extraction

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ test.jsonl     â”‚ â†’ NERâ”‚ en_ner_bc5cdr_md      â”‚ â†’ âœ‚ï¸ â”‚ clean_diseases.blacklist() â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

* raw SciSpaCy mentions â†’ `output/diseases.jsonl`
* optional cleanâ€‘up â†’ `output/diseases_clean.jsonl`

### 3.4Â Evaluation

`evaluate.py` reloads both checkpoints and prints Accuracy, F1 and the confusion matrices sideâ€‘byâ€‘side.

---

## 4Â .Â Directory layout

```text
velsera/
â”œâ”€â”€ data/
â”‚Â Â  â”œâ”€â”€ raw/                 # original 1Â 000 abstracts (zip provided)
â”‚Â Â  â””â”€â”€ processed/           # parquet + jsonl splits
â”œâ”€â”€ models/                  # baseline/  |  lora_finetuned/
â”œâ”€â”€ output/                  # diseases.jsonl  |  diseases_clean.jsonl
â”œâ”€â”€ *.py                     # pipeline scripts
â””â”€â”€ requirements.txt
```

---

## 5Â .Â TroubleshootingÂ ğŸ©º

| symptom                                      | fix                                                                                                 |
| -------------------------------------------- | --------------------------------------------------------------------------------------------------- |
| **Pydantic TypeError** during `spacy.load()` | Youâ€™re on spaCyâ€¯â‰¥â€¯3.7 (Pydanticâ€‘v2). Reâ€‘install with `pip install "spacy==3.4.4" "scispacy==0.5.1"` |
| **404** when downloading the SciSpaCy model  | Use the S3 link above (AllenAI moved from GitHub releases)                                          |
| **CUDA wanted** warnings                     | The pipeline is CPUâ€‘friendly; ignore or set `CUDA_VISIBLE_DEVICES=""`                               |
| `FileNotFoundError: output/...`              | `mkdir -p output` or let the script create it (`Path().parent.mkdir(exist_ok=True)`)                |

---

## 6Â .Â Extending

* **Bigger models** â€“ swap `distilbert-base-uncased` for any HF seqâ€‘cls checkpoint.
* **Label set** â€“ add more folders under `data/raw/`, rerun `preprocess.py`.
* **Better cleaning** â€“ edit `clean_diseases.py`: add blackâ€‘list terms, acronym map, `set()` dedup.

Contributions & questions welcome â€“ open an issue or ping the maintainer.
