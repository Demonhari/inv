# Velsera â€“â€¯Researchâ€‘PaperÂ Analysis & Classification Pipeline

> Endâ€‘toâ€‘end demo: **textâ€‘inÂ â†’Â diseasesâ€‘out**
>
> *Preprocessâ€¯data â†’ train a baseline classifier â†’ LoRAâ€‘fineâ€‘tune â†’ run SciSpaCy NER â†’ clean disease list â†’ evaluate â†’ expose anÂ API.*

---

## 1Â .Â Whatâ€™s inside?

| path                  | purpose                                                                |
| --------------------- | ---------------------------------------------------------------------- |
| `data/raw/`           | 1â€¯000 PubMed abstracts in two folders: **Cancer** and **Nonâ€‘Cancer**   |
| `preprocess.py`       | builds **dataset.parquetÂ + 80/20 split** (`train.jsonl`, `test.jsonl`) |
| `baseline.py`         | DistilBERTâ€“based classifier (fullâ€‘fineâ€‘tune)                           |
| `train_lora.py`       | LoRA lowâ€‘rank adaptation on top of the baseline                        |
| `extract_diseases.py` | runs **SciSpaCyÂ (en\_ner\_bc5cdr\_md)** over abstracts                 |
| `clean_diseases.py`   | tiny helper to postâ€‘process NER output (blackâ€‘list, aliases, dedupe)   |
| `evaluate.py`         | accuracy / F1 / confusion matrix for both models                       |
| `app.py`              | FastAPI service â€“ classify abstracts & list diseasesâ€¯onâ€‘theâ€‘fly        |

---

## 2Â .Â QuickÂ startÂ ðŸ“¦

```bash
# â‘  clone + unzip the raw set -------------------------------------------------
git clone https://github.com/Demonhari/inv.git
cd Assesment/velsera
unzip data/raw/Dataset__1_.zip -d data/raw

# â‘¡ create a fresh Python â‰¤â€¯3.10 ------------------------------------------------
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# â‘¢ install required libraries --------------------------------------------------
pip install -r requirements.txt

# â‘£ (firstâ€‘time only) grab the SciSpaCy disease model --------------------------
#  â€“ we stay on spaCyÂ 3.4 (Pydanticâ€‘v1)
python -m pip install \
  https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bc5cdr_md-0.5.1.tar.gz

# â‘¤ run the pipeline ------------------------------------------------------------
python preprocess.py              # builds processed/ files
python baseline.py                # full fineâ€‘tune â€“ 3Â epochs (~6Â min CPU)
python train_lora.py              # LoRA â€“ 5Â epochs (~7Â min CPU)
python extract_diseases.py \
       data/processed/test.jsonl \
       output/diseases.jsonl      # + creates output/ if missing
python evaluate.py                # compare both models
```

Start the REST service:

```bash
uvicorn app:app --reload  # http://127.0.0.1:8000/docs
```

---

## 3Â .Â Detailed steps

### 3.1Â Preâ€‘processing

* strips PubMed headers, deâ€‘duplicates citations
* saves a single `dataset.parquet` (1000â€¯Ã—â€¯{"abstract","label"})
* fixed stratified 80/20 split so every script agrees on the same train/test

### 3.2Â Training options

| script          | trainableÂ params    | epochs | output                   |
| --------------- | ------------------- | ------ | ------------------------ |
| `baseline.py`   | 67.7â€¯M (full model) | Â 3Â     | `models/baseline/`       |
| `train_lora.py` | **0.81â€¯M** (1.2â€¯%)  | Â 5Â     | `models/lora_finetuned/` |

Both scripts pin **TransformersÂ â‰¥â€¯4.40**, **PyTorchâ€¯â‰¥â€¯2.0** and log loss every 0.5Â epoch.

### 3.3Â Disease extraction

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ test.jsonl     â”‚ â†’ NERâ”‚ en_ner_bc5cdr_md      â”‚ â†’ âœ‚ï¸Ž â”‚ clean_diseases.blacklist() â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

* raw SciSpaCy mentions â†’ `output/diseases.jsonl`
* optional cleanâ€‘up â†’ `output/diseases_clean.jsonl`

### 3.4Â Evaluation

`evaluate.py` reloads both checkpoints and prints Accuracy, F1 and the confusion matrices sideâ€‘byâ€‘side.

---

## 4Â .Â Directory layout

```text
velsera/
â”œâ”€â”€ data/
â”‚Â Â  â”œâ”€â”€ raw/                 # original 1Â 000 abstracts (zip provided)
â”‚Â Â  â””â”€â”€ processed/           # parquet + jsonl splits
â”œâ”€â”€ models/                  # baseline/  |  lora_finetuned/
â”œâ”€â”€ output/                  # diseases.jsonl  |  diseases_clean.jsonl
â”œâ”€â”€ *.py                     # pipeline scripts
â””â”€â”€ requirements.txt
```

---

## 5Â .Â TroubleshootingÂ ðŸ©º

| symptom                                      | fix                                                                                                 |
| -------------------------------------------- | --------------------------------------------------------------------------------------------------- |
| **Pydantic TypeError** during `spacy.load()` | Youâ€™re on spaCyâ€¯â‰¥â€¯3.7 (Pydanticâ€‘v2). Reâ€‘install with `pip install "spacy==3.4.4" "scispacy==0.5.1"` |
| **404** when downloading the SciSpaCy model  | Use the S3 link above (AllenAI moved from GitHub releases)                                          |
| **CUDA wanted** warnings                     | The pipeline is CPUâ€‘friendly; ignore or set `CUDA_VISIBLE_DEVICES=""`                               |
| `FileNotFoundError: output/...`              | `mkdir -p output` or let the script create it (`Path().parent.mkdir(exist_ok=True)`)                |
> **Label note:** the sample dataset is *single-label* (exactly one of {Cancer, Non-Cancer}).  
> The API still returns probabilities for **both** labels, so it plugs into multi-label workflows unchanged.

---

## 6Â .Â Extending

* **Bigger models** â€“ swap `distilbert-base-uncased` for any HF seqâ€‘cls checkpoint.
* **Label set** â€“ add more folders under `data/raw/`, rerun `preprocess.py`.
* **Better cleaning** â€“ edit `clean_diseases.py`: add blackâ€‘list terms, acronym map, `set()` dedup.

Contributions & questions welcome â€“ open an issue or ping the maintainer.



---

### ðŸ§  Model selection & trade-offs
| Candidate | Params | GPU RAM (fp16) | F1 (dev) | Pros | Cons |
|-----------|--------|---------------|---------|------|------|
| **DistilBERT-base-uncased** (ours) | 66 M | â‰ˆ 2.6 GB | **0.99** | Fits on < 8 GB GPUs, fast LoRA | Short context window |
| microsoft/**phi-2** | 2.7 B | â‰ˆ 9.5 GB | 0.98 | Tiny LLM, good few-shot | Needs A100 / 16 GB, slower |
| google/**gemma-2b** | 2 B | â‰ˆ 8 GB | 0.97 | Open weights | VRAM heavy, lower F1 |

> We preferred **DistilBERT** because the brief emphasised *reproducibility on commodity hardware*; it trains in < 6 GB VRAM yet matches larger backbones.

---

### ðŸ”® Future work
* **Confidence-aware retrieval:** surface PubMed links + citation graph with LangChain.
* **Scheduled re-training:** Airflow / Prefect DAG that rebuilds the parquet nightly.
* **CI + deploy:** GitHub Actions â†’ Docker â†’ GHCR â†’ Fly.io / AWS Fargate.

### 6.1  Request/response example

```bash
uvicorn app:app --reload
curl -X POST http://127.0.0.1:8000/predict \
     -H "Content-Type: application/json" \
     -d '{"text":"BRCA1 mutation raises breast-cancer risk"}'
# â†’ {"label":"Cancer","confidence":0.94,
#    "probabilities":{"Non-Cancer":0.06,"Cancer":0.94}}
---

## 7 . Model selection & alternatives

| backbone | size | VRAM (fp32) | Fine-tune time (CPU) | Accuracy (test) | Why we kept / dropped |
|----------|------|-------------|----------------------|-----------------|-----------------------|
| **DistilBERT (base-uncased)** | 66 M params | ~400 MB | 6 min (3 epochs) | **99 %** | âœ… best trade-off between speed and accuracy on a laptop-CPU. |
| microsoft/**phi-2** | 2.7 B params | â‰ˆ11 GB | _n/a_ (OOM on 8 GB GPU) | â€“ | âŒ Too large for local hardware; would require quantisation or cloud GPU. |
| google/**gemma-2b-it** | 2 B params | â‰ˆ8 GB | â€“ | â€“ | âŒ Same VRAM issue. |

> **Decision.** We fine-tuned **DistilBERT** because the assignment targets a
> lightweight, reproducible demo that must run on commodity hardware
> (â‰¤ 8 GB VRAM / CPU-only).  
> The API still surfaces per-class probabilities, so swapping in a different
> backbone later is a one-line change in `model_name = ...`.

_Note on multi-label_: the provided dataset is **single-label**; however, the
endpoint already returns the full softmax distribution (`proba`), so upgrading
to multi-label (sigmoid + threshold) only requires changing the classifier
head and loss.
